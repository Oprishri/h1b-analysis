// in beeline:
CREATE TABLE employer_name_job_title AS SELECT employer_name, job_title FROM data20;

// export it to dumbo then to hdfs
beeline -u jdbc:hive2://babar.es.its.nyu.edu:10000/ -n my1843 -w code --outputformat=tsv2 -e "use my1843; select * from employer_name_job_title" >> distinct_employer_name.tsv

//check
cat distinct_employer_name.tsv | head -n 10

// put it to hdfs
hdfs dfs -put distinct_employer_name.tsv /user/my1843/project

// run map reduce on distinct_employer_name.tsv
*********>>>>>>> after running the wordCount map reduce

//
create external table employer_name_count(employer_name string, count_name double) row format delimited fields terminated by '\t' location '/user/my1843/project/wordCount';

//
CREATE TABLE employer_name_count_order AS SELECT employer_name, count_name FROM employer_name_count ORDER BY count_name DESC;

select * from employer_name_count_order limit 100;
select count(*) employer_name_count_order;

// export to dumbo
beeline -u jdbc:hive2://babar.es.its.nyu.edu:10000/ -n my1843 -w code --outputformat=tsv2 -e "use my1843; select * from employer_name_count_order" >> employer_name_count_order.tsv

// put the high frequency keyowrds into category by eye ball.

// run another map reduce to add one category column to data
